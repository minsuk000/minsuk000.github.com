---
layout: post
title: "The Optimality Of Posterior Contraction Rate and Priors"
comments: true
---

Recently, one of my friends asked me a question about the previous post "[How to prove the  Bayesian Model Selection Consistency in Linear Models for High-dimensional Settings]({% post_url 2016-06-14-213921%})". He was wondering the following statement in the post.

>Even though Gaussian tails hurt the optimal minimax rate of posterior contraction, it is ok when we only consider model selections.

He asked me why the use of priors with Gaussian tails hurt the optimal rate of its posterior contraction rate. A density with a Gaussian tail refers to a density function that decays a square-exponential rate, i.e., $$-\log\pi(\theta)= O(\theta^2)$$. 

The basic idea is the following. Consider a simple model where $$y_i\mid \theta \overset{i.i.d.}{\sim}N(\theta,1)$$ for $$i=1,\dots,n$$. The naive choice of the prior can be $$\theta \sim N(0,\tau^2)$$. The resulting posterior mean is 

$$
E(\theta\mid y) = \frac{n}{n+\tau^2}\bar y,
$$ 

where $$\bar y$$ is the sample mean. The $$L_2$$ risk can be expressed as

$$
\begin{eqnarray*}
E_{\theta_0}[\{\theta_0 - E(\theta\mid y)\}^2] &=& E_{\theta_0}[(\theta_0-\bar y)^2] + E_{\theta_0}[\{\bar y-E(\theta\mid y)\}^2]\\
&=& \frac{1}{n} + \frac{1}{n(n+\tau^2)^2}+\frac{\theta_0^2}{(n+\tau^2)^2}, 
\end{eqnarray*}
$$

where $$\theta_0$$ is the true parameter that involves in the data-generating process and $$E_{\theta_0}(\cdot)$$ refers to the expectation operator corresponding to $$\theta_0$$. Therefore, the resulting supremum of the risk with respect to $$\theta_0$$ is 

$$
\begin{eqnarray*}
\sup_{\theta_0} E_{\theta_0}[\{\theta_0 - E(\theta\mid y)\}^2] &=& \frac{1}{n} + \frac{1}{n(n+\tau^2)}+\sup_{\theta_0}\left(\frac{\theta_0^2}{n+\tau^2} \right)\\
&=& \infty.
\end{eqnarray*}
$$

The supremum of the $$L_2$$ risk is infinity, and the resulting minimax rate of the posterior contraction  fails to achieve the optimal rate. This result also can be applied to more general settings. That's why I mentioned in the previous post that Gaussian tails could hurt the minimax rate of the posterior contraction.

On the other hand, when we consider priors that have tails at least as heavy as exponential distributions, i.e., $$-\log\pi(\theta)= O(\vert\theta\vert)$$, one can show that the resulting posterior mean satisfies the following:

$$
\sup_{\theta_0} E_{\theta_0}[\{\theta_0 - E(\theta\mid y)\}^2] \leq C,
$$ 
 
for some positive constant $$C$$. This is a key ingredient to prove the optimal minimax rate of posterior contraction. If you're interested in details, please see [Castillo and van der Vaart (2011)](https://projecteuclid.org/euclid.aos/1351602537). This paper is about high-dimensional sparse settings, but it also well-describes the idea of posterior contractions.   



