---
layout: post
title: "Simplified Shotgun Stochastic Search with Screening (S5)"
date: 2016-10-27 20:41
hidden: true
---

### DRAFT


Today I am going to introduce my recent work about a scalable stochastic search algorithm for high-dimensional Bayesian variable selection, which is called "Simplified Shotgun Stochastic Search with Screening" (S5). This algorithm is not only super-fast and efficient, but it is also scalable in a sense that the computational cost is robust to the dimension $$p$$.  

### Motivation

I was writing a paper about [a high-dimensional Bayesian variable selection method](http://www.stat.tamu.edu/~minsuk/publications/nonlocal_sinica7.dpf). I was so naive to try to examine 3 settings with different 40 tuning parameters and 100 datasets with $$p $$ =20000; total 12,000 high-dimensional  datasets. I found that for a each dataset it was expected to take about **2 hours** and the total simulation time was estimated about 1,000 days. Even if I used multiple cores for the computation, it was still not feasible. Practically to do the simulation, I needed some algorithm not only giving me results within **10 seconds**, but also providing an efficient uncertainty identification of the model space.

### Model

Consider a linear regression model given a model $${\bf k}$$ as 
 
$$
 y = X_{\bf k}\beta_{\bf k} + \epsilon,
 $$
 
 where $$\epsilon \sim N(0,\sigma^2I_n)$$ and $$ X_{\bf k} $$ refers to a submatrix of $$X$$ containing the variables in the model $${\bf k}$$. The resulting posterior probability of each model  is 

$$
\pi({\bf k}\mid y) = \frac{m_{\bf k}(y)\pi({\bf k})}{\sum_{\bf l}m_{\bf l}(y)\pi({\bf l})}, 
$$  

 where $$m_{\bf k}(y)$$ is the marginal likelihood of the model $${\bf k}$$ and $$\pi({\bf k})$$ is its model prior.

However, the problem is that there exist too many possible models; the total number of possible models is $$2^p$$. Even when $$p$$ is moderate, saying 50, we cannot evaluate $$2^50$$ number of marginal likelihoods. 

### Issues
 *Markov Chain Monte Carlo* (MCMC) has been popular to approximate the posterior model probability. However, the fact that the computational efficiency of MCMC is not scalable (heavily depends on the dimension $$p$$ and length of the chain) hinders the usage of the algorithm in high-dimensional settings. To overcome this issue, [George and Rockova (2015)](http://www.tandfonline.com/doi/abs/10.1080/01621459.2013.869223?journalCode=uasa20) proposed a deterministic approach using the *Expectation-Maximization* (EM) algorithm, but it can be only applied to continous mixtures of spike and slab priors, while the above procedure is with discrete mixtures. Moreover, the interpretation of the approximated latent variable by the EM algorithm is not clear, since it actually does not fully incorporate the posterior model probability. 
 
 ### Shotgun Stochastic Search (SSS)
   







  





  

