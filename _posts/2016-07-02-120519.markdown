---
layout: post
title: "Penalized Likelihood Estimators and Shrinkage Priors: LASSO and Bayesian LASSO"
comments: true
---

Penalized likelihood methods have been popular in many statistical inference problems. The advantage of using penalized likelihood estimators is that it could reduce  the Mean Square Error (MSE) by balancing the bias-variance trade-off. Even though unbiased estimators or asymptotically unbiased estimators (such as maximum likelihood estimators) have desirable properties, the  variance of those estimators could be undesirably large so that the resulting MSE is large too. The idea of the penalized likelihood estimator is to reduce the variance by compromising the bias of the estimator.  

Suppose that the likelihood of the observed data is denoted by $$L(y\mid\theta)$$  with the parameter of interest $$\theta$$. The estimator can be expressed in the following:

$$
\widehat{\theta} = \underset{\theta}{\operatorname{argmin}} -2\times \log L(y\mid \theta) + p_\lambda(\theta),
$$   

 where $$p_\lambda(\cdot)$$ is the penalty function with the tuning parameter $$\lambda$$ that controls the strength of the penalty. The only difference between the maximum likelihood estimator and the penalized maximum likelihood estimator is that the objective function of the penalized version contains the penalty part $$p_\lambda(\theta)$$, while that of the maximum likelihood estimator doesn't. 
 
 Let me give you a simple example of this idea. Suppose $$y_i \overset{i.i.d.}{\sim} N(\theta,1)$$ for $$i=1,\dots,n$$. Consider three penalties: $$L_0$$ penalty ($$p_\lambda(\theta) = \lambda I( \theta\neq 0 )$$), $$L_1$$ penalty ($$p_\lambda(\theta) = \lambda \vert\theta\vert$$) and  $$L_2$$ penalty ($$p_\lambda(\theta) = \lambda \theta^2$$), where $$I(\cdot)$$ is the indicator function. Then the resulting objective function of each estimator can be expressed as
 
 $$
 \begin{eqnarray*}
 \mbox{$L_0$ penalty: } &&\widehat \theta_{L_0} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda I(\theta \neq 0)\\
 \mbox{$L_1$ penalty: } && \widehat \theta_{L_1} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda |\theta|\\
 \mbox{$L_2$ penalty: } && \widehat \theta_{L_2} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
 $$
 
 Equivalently, since $$\sum_{i=1}^n  (y_i-\theta)^2 = \sum_{i=1}^n(y_i -\bar y)^2+ n (\bar y -\theta)^2$$, where $$\bar y$$ is the sample mean, the above estimators  can be expressed as 
  
$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda I(\theta \neq 0)\\
   \widehat \theta_{L_1} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda |\theta|\\
 \widehat \theta_{L_2} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
$$
  
The resulting estimators have the explicit forms as 

$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \begin{cases} 0,\:\:\mbox{ if $|\bar y|< (\lambda/n)^{1/2} $}\\
 \bar y,\:\:\mbox{ if $|\bar y|\geq (\lambda/n)^{1/2} $}
 \end{cases}
 ,\\

   \widehat \theta_{L_1} &=&  \begin{cases} 0,\:\:\mbox{ if $|\bar y|< \lambda/n $}\\
 \bar y - \frac{\lambda}{n}sgn(\bar y),\:\:\mbox{ if $|\bar y|\geq \lambda/n $}
 \end{cases}
, \\

 \widehat \theta_{L_2} &=& \frac{n}{n+\lambda}\bar y,
 \end{eqnarray*}
$$

where $$sgn(\cdot)$$ is the sign function. 

To help the understainding, I attached Figure 1 that illustrates the above estimators according to varying $$\bar y$$.

##### Figure 1: the sample mean $$\bar y$$ versus the penalized estimators $$\widehat \theta$$: $$L_0$$ penalized estimator (left), $$L_1$$ penalized estimator (middle), and $$L_2$$ penalized estimator (right).

![look at me](/assets/penalty2.pdf)

 It is clear that the $$L_2$$ penalized estimator (right) is not a sparse estimator, which could result in an exactly zero estimated value, since  the estimator $$\theta_{L_2}$$ is always non-zero unless the $$\bar y$$ is exactly zero. On the other hand, the $$L_0$$ (left) and $$L_1$$ (middle) penalized estimators are sparse in the sense that the resulting estimators could be exactly zero when the $$\vert \bar y \vert$$ is small enough. The $$L_0$$ penalized estimator is called a **hard-thresholding** estimator, since the estimated value keeps the $$\bar y$$, if the $$\vert\bar y\vert$$ is larger than a specificc threshold $$\lambda/n$$, and it becomes exactly zero, otherwise. One important feature of the hard-thresholding estimator is that the solution function is not continuous, and you can see the gap on the function in Figure 1 (left), at $$-(\lambda/n)^{1/2}$$ and $$(\lambda/n)^{1/2}$$. On the other hand, the $$L_1$$ estimator is called  **soft-thresholded**, and it is different from the $$L_2$$ estimator (hard-thresholded) in a sense that there always exists a bias of the estimator, which means that even when the $$\bar y$$ is far enough from zero the soft-thresholded estimator is differentiated from the $$\bar y$$. However, the $$L_0$$ penalized estimator is exactly equivalent to the $$\bar y$$ when $$\vert\bar y \vert\geq(\lambda/n)^{1/2}$$.
 
#### Which one is better? $$L_0$$, $$L_1$$, or $$L_2$$ ?

 In the previous section, I mentioned that the $$L_0$$ and $$L_1$$ penalized estimators are sparse estimators, but the $$L_2$$ penalized estimator is not. The sparsity is useful when we have to consider model selection problems, since in many settings a sparsity pattern of the parameters determines a model, for example in linear regression models the zero regression parameters indicates that the corresponding independent variables are irrelavant to the response in the regression model. Then, next natural question is "**Which one is better among the $$L_0$$ and $$L_1$$ penalized estimators?**" 
 
 My answer is that the $$L_0$$ is better, if it's computationally possible.   
  
 The $$L_0$$ penalized estimator enjoys some desirable properties such as the oracle properties: [Yongdai Kim](http://oldstat.snu.ac.kr/ydkim/)'s great explanation of this concept is in [this slides](https://www.birs.ca/workshops/2011/11w5051/files/12_yongdai_kim_on_weak_and_strong_oracle_property.pdf), while the $$L_1$$ estimator fails to achieve this in linear model settings, since the $$L_1$$ penalized estimator is baised. However, when we consider a large number of  parameters, sometimes larger than the sample size, it is computationally very demanding to find the solution of the $$L_0$$ penalized objective function; it is a so-called NP-hard problem. On the other hand, the $$L_1$$ penalty function $$\vert\theta\vert$$ is convex for multi-dimensional $$\theta \in \mathbb{R}^d$$ for some large positive integer $$d$$, the resulting optimization is also convex when the $$-2\times \log L(y\mid \theta)$$ is convex. Convex optimizations not only quarantee to search the global minimizer, but also there are a rich amount of reliable optimization algorithms to attack this problem.    
 
 
####
 
 
 
 
 
 





