---
layout: post
title: "Penalized Likelihood Estimators and Shrinkage Priors: LASSO and Bayesian LASSO"
comments: true
---

Penalized likelihood methods have been popular in many statistical inference problems. The advantage of using penalized likelihood estimators is that it could reduce  the Mean Square Error (MSE) by balancing the bias-variance trade-off. Even though unbiased estimators or asymptotically unbiased estimators (such as maximum likelihood estimators) have desirable properties, the  variance of those estimators could be undesirably large so that the resulting MSE is large too. The idea of the penalized likelihood estimator is to reduce the variance by compromising the bias of the estimator.  
Suppose that the likelihood of the observed data is denoted by $$L(y\mid\theta)$$  with the parameter of interest $$\theta$$. The estimator can be expressed in the following:

$$
\widehat{\theta} = \underset{\theta}{\operatorname{argmin}} -2L(y\mid \theta) + p_\lambda(\theta),
$$   

 where $$p_\lambda(\cdot)$$ is the penalty function with the tuning parameter $$\lambda$$ that controls the strength of the penalty. The only difference between the maximum likelihood estimator and the penalized maximum likelihood estimator is that the objective function of the penalized version contains the penalty part $$p_\lambda(\theta)$$, while that of the maximum likelihood estimator doesn't. 
 
 Let me give you a simple example of this idea. Suppose $$y_i \overset{i.i.d.}{\sim} N(\theta,1)$$ for $$i=1,\dots,n$$. Consider three penalties: $$L_0$$ penalty ($$p_\lambda(\theta) = \lambda I( \theta\neq 0 )$$), $$L_1$$ penalty ($$p_\lambda(\theta) = \lambda \vert\theta\vert$$) and  $$L_2$$ penalty ($$p_\lambda(\theta) = \lambda \theta^2$$), where $$I(\cdot)$$ is the indicator function. Then the resulting objective function of each estimator can be expressed as
 
 $$
 \begin{eqnarray*}
 \mbox{$L_0$ penalty: } &&\widehat \theta_{L_0} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda I(\theta \neq 0)\\
 \mbox{$L_1$ penalty: } && \widehat \theta_{L_1} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda |\theta|\\
 \mbox{$L_2$ penalty: } && \widehat \theta_{L_2} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
 $$
 
 Equivalently, since $$\sum_{i=1}^n  (y_i-\theta)^2 = \sum_{i=1}^n(y_i -\bar y)^2+ n (\bar y -\theta)^2$$, the above estimatros  can be expressed as 
  
$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda I(\theta \neq 0)\\
   \widehat \theta_{L_1} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda |\theta|\\
 \widehat \theta_{L_2} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
 $$
  



