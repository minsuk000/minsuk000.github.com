---
layout: post
title: "Penalized Likelihood Estimators and Shrinkage Priors: LASSO and Bayesian LASSO"
comments: true
---

Penalized likelihood methods have been popular in many statistical inference problems. The advantage of using penalized likelihood estimators is that it could reduce  the Mean Square Error (MSE) by balancing the bias-variance trade-off. Even though unbiased estimators or asymptotically unbiased estimators (such as maximum likelihood estimators) have desirable properties, the  variance of those estimators could be undesirably large so that the resulting MSE is large too. The idea of the penalized likelihood estimator is to reduce the variance by compromising the bias of the estimator.  

Suppose that the likelihood of the observed data is denoted by $$L(y\mid\theta)$$  with the parameter of interest $$\theta$$. The estimator can be expressed in the following:

$$
\widehat{\theta} = \underset{\theta}{\operatorname{argmin}} -2\times L(y\mid \theta) + p_\lambda(\theta),
$$   

 where $$p_\lambda(\cdot)$$ is the penalty function with the tuning parameter $$\lambda$$ that controls the strength of the penalty. The only difference between the maximum likelihood estimator and the penalized maximum likelihood estimator is that the objective function of the penalized version contains the penalty part $$p_\lambda(\theta)$$, while that of the maximum likelihood estimator doesn't. 
 
 Let me give you a simple example of this idea. Suppose $$y_i \overset{i.i.d.}{\sim} N(\theta,1)$$ for $$i=1,\dots,n$$. Consider three penalties: $$L_0$$ penalty ($$p_\lambda(\theta) = \lambda I( \theta\neq 0 )$$), $$L_1$$ penalty ($$p_\lambda(\theta) = \lambda \vert\theta\vert$$) and  $$L_2$$ penalty ($$p_\lambda(\theta) = \lambda \theta^2$$), where $$I(\cdot)$$ is the indicator function. Then the resulting objective function of each estimator can be expressed as
 
 $$
 \begin{eqnarray*}
 \mbox{$L_0$ penalty: } &&\widehat \theta_{L_0} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda I(\theta \neq 0)\\
 \mbox{$L_1$ penalty: } && \widehat \theta_{L_1} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda |\theta|\\
 \mbox{$L_2$ penalty: } && \widehat \theta_{L_2} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
 $$
 
 Equivalently, since $$\sum_{i=1}^n  (y_i-\theta)^2 = \sum_{i=1}^n(y_i -\bar y)^2+ n (\bar y -\theta)^2$$, where $$\bar y$$ is the sample mean, the above estimators  can be expressed as 
  
$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda I(\theta \neq 0)\\
   \widehat \theta_{L_1} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda |\theta|\\
 \widehat \theta_{L_2} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
$$
  
The resulting estimators have the explicit forms as 

$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \begin{cases} 0,\:\:\mbox{ if $|\bar y|< (\lambda/n)^{1/2} $}\\
 \bar y,\:\:\mbox{ if $|\bar y|\geq (\lambda/n)^{1/2} $}
 \end{cases}
 ,\\

   \widehat \theta_{L_1} &=&  \begin{cases} 0,\:\:\mbox{ if $|\bar y|< \lambda/n $}\\
 \bar y - \frac{\lambda}{n}sgn(\bar y),\:\:\mbox{ if $|\bar y|\geq \lambda/n $}
 \end{cases}
, \\

 \widehat \theta_{L_2} &=& \frac{n}{n+\lambda}\bar y,
 \end{eqnarray*}
$$

where $$sgn(\cdot)$$ is the sign function. 

To help the understainding, I attached Figure 1 that illustrates the above estimators according to varying $$\bar y$$.

##### Figure 1: the sample mean $$\bar y$$ versus the penalized estimators $$\widehat \theta$$: $$L_0$$ penalized estimator (left), $$L_1$$ penalized estimator (middle), and $$L_2$$ penalized estimator (right).

![look at me](/assets/penalty2.pdf)

 It is clear that the $$L_2$$ penalized estimator (right) is not a sparse estimator, which could result in an exactly zero estimated value, since  the estimator $$\theta_{L_2}$$ is always non-zero unless the $$\bar y$$ is exactly zero. On the other hand, the $$L_0$$ (left) and $$L_1$$ (middle) penalized estimators are sparse in the sense that the resulting estimators could be exactly zero when the $$\vert \bar y \vert$$ is small enough. The $$L_0$$ penalized estimator is called a **hard-thresholding** estimator, since the estimated value keeps the $$\bar y$$, if the $$\vert\bar y\vert$$ is larger than a specificc threshold $$\lambda/n$$, and it becomes exactly zero, otherwise. One important feature of the hard-thresholding estimator is that the solution function is not continuous, and you can see the gap on the function in Figure 1 (left), at $$-\lambda/n$$ and $$\lambda/n$$. On the other hand, the $$L_1$$ estimator is called  **soft-thresholded**, and it is different from the $$L_2$$ estimator  
  
 





