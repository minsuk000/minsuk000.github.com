---
layout: post
title: "Penalized Likelihood Estimators and Shrinkage Priors (1)"

comments: true
---

Penalized likelihood methods have been popular in many statistical inference problems. The advantage of using penalized likelihood estimators is that it could reduce  the Mean Square Error (MSE) by balancing the bias-variance trade-off. Even though unbiased estimators or asymptotically unbiased estimators (such as maximum likelihood estimators) have desirable properties, the  variance of those estimators could be undesirably large so that the resulting MSE is large too. The idea of the penalized likelihood estimator is to reduce the variance by compromising the bias of the estimator.  

Suppose that the likelihood of the observed data is denoted by $$L(y\mid\theta)$$  with the parameter of interest $$\theta$$. The estimator can be expressed in the following:

$$
\widehat{\theta} = \underset{\theta}{\operatorname{argmin}} -2\times \log L(y\mid \theta) + p_\lambda(\theta),
$$   

 where $$p_\lambda(\cdot)$$ is the penalty function with the tuning parameter $$\lambda$$ that controls the strength of the penalty. The only difference between the maximum likelihood estimator and the penalized maximum likelihood estimator is that the objective function of the penalized version contains the penalty part $$p_\lambda(\theta)$$, while that of the maximum likelihood estimator doesn't. 
 
 Let me give you a simple example of this idea. Suppose $$y_i \overset{i.i.d.}{\sim} N(\theta,1)$$ for $$i=1,\dots,n$$. Consider three penalties: $$L_0$$ penalty ($$p_\lambda(\theta) = \lambda I( \theta\neq 0 )$$), $$L_1$$ penalty ($$p_\lambda(\theta) = \lambda \vert\theta\vert$$) and  $$L_2$$ penalty ($$p_\lambda(\theta) = \lambda \theta^2$$), where $$I(\cdot)$$ is the indicator function. Then the resulting objective function of each estimator can be expressed as
 
 $$
 \begin{eqnarray*}
 \mbox{$L_0$ penalty: } &&\widehat \theta_{L_0} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda I(\theta \neq 0)\\
 \mbox{$L_1$ penalty: } && \widehat \theta_{L_1} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda |\theta|\\
 \mbox{$L_2$ penalty: } && \widehat \theta_{L_2} =  \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^n  (y_i-\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
 $$
 
 Equivalently, since $$\sum_{i=1}^n  (y_i-\theta)^2 = \sum_{i=1}^n(y_i -\bar y)^2+ n (\bar y -\theta)^2$$, where $$\bar y$$ is the sample mean, the above estimators  can be expressed as 
  
$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda I(\theta \neq 0)\\
   \widehat \theta_{L_1} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda |\theta|\\
 \widehat \theta_{L_2} &=&  \underset{\theta}{\operatorname{argmin}} n (\bar y -\theta)^2 + \lambda \theta^2.
 \end{eqnarray*}
$$
  
The resulting estimators have the explicit forms as 

$$
 \begin{eqnarray*}
 \widehat \theta_{L_0} &=&  \begin{cases} 0,\:\:\mbox{ if $|\bar y|< (\lambda/n)^{1/2} $}\\
 \bar y,\:\:\mbox{ if $|\bar y|\geq (\lambda/n)^{1/2} $}
 \end{cases}
 ,\\

   \widehat \theta_{L_1} &=&  \begin{cases} 0,\:\:\mbox{ if $|\bar y|< \lambda/n $}\\
 \bar y - \frac{\lambda}{n}sgn(\bar y),\:\:\mbox{ if $|\bar y|\geq \lambda/n $}
 \end{cases}
, \\

 \widehat \theta_{L_2} &=& \frac{n}{n+\lambda}\bar y,
 \end{eqnarray*}
$$

where $$sgn(\cdot)$$ is the sign function. 

To help the understanding, I attached Figure 1 that illustrates the above estimators according to varying $$\bar y$$.

##### Figure 1: the sample mean $$\bar y$$ versus the penalized estimators $$\widehat \theta$$: $$L_0$$ penalized estimator (left), $$L_1$$ penalized estimator (middle), and $$L_2$$ penalized estimator (right).

![look at me](/assets/penalty2.pdf)

 It is clear that the $$L_2$$ penalized estimator (right) is not a sparse estimator, which could result in an exactly zero estimated value, since  the estimator $$\theta_{L_2}$$ is always non-zero unless the $$\bar y$$ is exactly zero. On the other hand, the $$L_0$$ (left) and $$L_1$$ (middle) penalized estimators are sparse in the sense that the resulting estimators could be exactly zero when the $$\vert \bar y \vert$$ is small enough. The $$L_0$$ penalized estimator is called a **hard-thresholding** estimator, since the estimated value keeps the $$\bar y$$, if the $$\vert\bar y\vert$$ is larger than a specificc threshold $$\lambda/n$$, and it becomes exactly zero, otherwise. One important feature of the hard-thresholding estimator is that the solution function is not continuous, and you can see the gap on the function in Figure 1 (left), at $$-(\lambda/n)^{1/2}$$ and $$(\lambda/n)^{1/2}$$. On the other hand, the $$L_1$$ estimator is called  **soft-thresholding**, and it is different from the $$L_0$$ estimator (hard-thresholded) in a sense that there always exists a bias of the estimator, which means that even when the $$\bar y$$ is far enough from zero the soft-thresholded estimator is differentiated from the $$\bar y$$. However, the $$L_0$$ penalized estimator is exactly equivalent to the $$\bar y$$ when $$\vert\bar y \vert\geq(\lambda/n)^{1/2}$$.
 

 In the previous section, I mentioned that the $$L_0$$ and $$L_1$$ penalized estimators are sparse estimators, but the $$L_2$$ penalized estimator is not. The sparsity is useful when we have to consider model selection problems, since in many settings a sparsity pattern of the parameters determines a model, for example in linear regression models the zero regression parameters indicate that the corresponding independent variables are irrelavant to the response in the regression model. Then, natural question is "**Which one is better among the $$L_0$$ and $$L_1$$ penalized estimators?**" 
 
 *My answer is that the $$L_0$$ is better than the $$L_1$$, if it's computationally possible.*   
  
 By choosing appropriate rate of $$\lambda$$, the $$L_0$$ penalized estimator enjoys some desirable properties such as the oracle properties, which means that the estimator achieves the same asymptotic behavior with that of the oracle estimator that is the estimator under the true sparsity pattern is known: [Yongdai Kim](http://oldstat.snu.ac.kr/ydkim/)'s great explanation of this concept is in [this slides](https://www.birs.ca/workshops/2011/11w5051/files/12_yongdai_kim_on_weak_and_strong_oracle_property.pdf), while the $$L_1$$ estimator fails to achieve this in linear model settings, since the $$L_1$$ penalized estimator is biased. However, when we consider a large number of  parameters, sometimes larger than the sample size, it is computationally very demanding to find the solution of the $$L_0$$ penalized objective function; it is so-called a NP-hard problem. On the other hand, the $$L_1$$ penalty function $$\vert\theta\vert$$ is convex for multi-dimensional $$\theta \in \mathbb{R}^d$$ for some large positive integer $$d$$, the resulting optimization is also convex when the $$-2\times \log L(y\mid \theta)$$ is convex. Convex optimizations not only guarantee the global minimizer, but also can be solved by a rich amount of reliable optimization algorithms, for example [the coordinate descent algorithms](https://arxiv.org/pdf/0803.3876.pdf), [the ADMM](http://arxiv.org/pdf/1208.3922v3.pdf), [the proximal algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf), and etc. For linear regression models with $$L_1$$ penalty (LASSO), [the Least Angle RegreSsion (LARS)](https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf) algorithm provides a very fast and scalable computation. The idea is that the authors captured he similarity between the forward selection and LASSO, and they showed that the forward selection can be used to approximate the LASSO solution.
 I will discuss the details in following posts.    
 
 
#### $$L_2$$ penalties are still useful in many settings. 

During the late 1950s,  statisticians got shocked by the introduction of an estimator called [James-Stein (JS) estimator](https://en.m.wikipedia.org/wiki/Jamesâ€“Stein_estimator). The reason was that under mild conditions the risk of the JS estimator is uniformly lower than that of the MLE in Gaussian sequence models, and it turned out that the JS estimator is a kind of $$L_2$$ penalized likelihood estimator. This is demonstrated again in [Efron and Morris (1973)](http://faculty.chicagobooth.edu/nicholas.polson/teaching/41900/efron-morris2.pdf) and  [Efron and Morris (1977)](http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf), and  it can be interpreted as an empirical Bayes estimator with a legendary example about baseball batting rates.

#### Conclusion

 I briefly talked about some penalized estimator, and simple discussion about pros and cons on the penalties. In the follwing post, I will discuss an interesting property of sparse estimators, which is called **super-efficiency**.  


 
 
 
 
 
 





