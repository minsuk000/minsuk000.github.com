---
layout: post
title: Model Confidence Bounds
date: 2016-11-27 12:24
tags: model selection
comments: true
---

 Yesterday, I found [an interesting paper](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwifzMa9hZXRAhUr3IMKHU1hAcQQFggjMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1611.09509&usg=AFQjCNHb5_bvsTP78rWBd3sNdb5ZIVrTlw&sig2=bn40mQJraf5feOrjhkXgmw) from arxiv. It is about *Model Confidence Bounds* which is an extension of confidence sets in real-valued parameters to model spaces for variable selection. Let me explain the details.

 Consider a linear regression model given a model $${\bf k}$$ as 
 
 $$
 y = X_{\bf k}\beta_{\bf k} + \epsilon,
 $$
 
Then, you might be interested in finding a set $${\bf k}$$ of significant predictors associated with $$y$$. Suppose that using an arbitrary variable selection procedure (LASSO or Bayesian model seleciton, etc), you have estimated a set of variables $$\widehat{\bf k}$$. Then, a natual question about this estimated model $$\widehat{\bf k}$$ is "How uncertain is the extimated model?". When the parameter is real-valued, there are many existing methods for estimating its confidence interval to measure the uncertainty of the estimation. However, in variable selection problems, it is not clear how to construct the confidence set for models  This question was first cast by [Hansen et al. (2011)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&sqi=2&ved=0ahUKEwj38KKYhZXRAhVkxYMKHWhhAq4QFggfMAA&url=http%3A%2F%2Fonlinelibrary.wiley.com%2Fdoi%2F10.3982%2FECTA5771%2Fabstract&usg=AFQjCNFVwcoWmHdDzWrZqJCeJzQruAIP3w&sig2=gsZlgfjqKx-dWlJ5dF9XAQ&cad=rjt). To overcome this issue, they proposed a procedure to construct a set of models that are statistically equivalent to an optimal model at a certain level of confidence $$1-\alpha$$. That is a confidence set for model selection problems. On the other hand, the authors of [the paper](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwifzMa9hZXRAhUr3IMKHU1hAcQQFggjMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1611.09509&usg=AFQjCNHb5_bvsTP78rWBd3sNdb5ZIVrTlw&sig2=bn40mQJraf5feOrjhkXgmw) have made a further step. They defined a lower model bound and an upper model bound for the model confidence set by using a variable ranking. I like this simple but nice idea.  

In Bayesian perspective, we have a natural measure of uncertainty of the model space:  posterior model probability as discussed in [the previous post]({% post_url 2016-06-14-213921 %}). Not only it results in simple weights for [Bayesian model averaging](http://www.stat.colostate.edu/~jah/papers/statsci.pdf), but it also gives us a simple intuition, e.g., [the posterior inclusion probability](https://arxiv.org/pdf/math/0406464.pdf). However, the behavior of the posterior model probability is very sensitive to the choice of priors. I think that one of the biggest weakness of Bayesian hypothesis testing  is that even uncertainty measurement heavily depends on priors and their hyperparameters; a reason why objective Bayesians are on a right way.



