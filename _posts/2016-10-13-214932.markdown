---
layout: post
title: "Penalized Likelihood Estimator and Shrinkage Priors (3): LASSO and Bayesian LASSO"
date: 2016-10-13 21:47
comments: true
---

### A Toy Example

Consider the following simple model that has been used in the previous post ["Penalized Likelihood Estimator and Shrinkage Priros (1)"]({% post_url 2016-07-02-120519%}):

$$
y_i  = \theta + \epsilon_i,
$$

where $$\epsilon_i\sim N(0,1) $$ for $$i=1,\dots,n$$.

One might be interested in the sparsity of $$ \theta $$; whether $$ \theta $$ is zero or not. In this simple example, we are able to use a hypothesis testing to make an inference on $$ \theta $$, but when we consider multiple parameters, it is not clear to infer the sparsity. This issue has motivated a use of penalty on the likelihood function.

### LASSO and Bayesian LASSO
Consider a $$L_1 $$ penalized likelihood estimator as 

$$
\widehat\theta_{L_1} = {argmin}_\theta \left\{ -2\log L(y\mid \theta) + \lambda|\theta| \right\} = argmin_\theta\left\{ \sum_{i=1}^n(y_i-\theta)^2 + \lambda|\theta| \right\},
$$

where $$ L(y \mid \cdot) $$ is the likelihood function and $$ \lambda $$ is the tuning parameter. Then, the resulting estimator has an explicit form as 

$$
\widehat\theta_{L_1} = \begin{cases} \bar y -\frac{\lambda}{n}sgn(\bar y),\:\:\mbox{if}\:\:|\bar y| \geq \frac{\lambda}{n}\\
0,\:\:\mbox{otherwise}, 
\end{cases}  
$$

where $$\bar y $$ is the sample mean and $$sgn(\cdot) $$ is the sign function. The solution is plotted in the previous post ["Penalized Likelihood Estimator and Shrinkage Priors (1)"]({% post_url 2016-07-02-120519%}).
 
This $$L_1 $$ penalty idea can be applied to a linear regression model, then the penalized likelihood method is called the ''Least Absolute Shrinkage and Selection Operator'' ([LASSO](http://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents)). [Park and Casella (2008)](http://www.stat.ufl.edu/~casella/Papers/Lasso.pdf) proposed a natural Bayesian version of LASSO by imposing  [the Laplace distribution](https://en.m.wikipedia.org/wiki/Laplace_distribution) (or double-exponential distribution) on the regession parameter. The resulting posterior distribution of $$ \theta$$ in the above simple model can be expresses as 

$$
 \pi(\theta \mid y) \propto L(y\mid\theta)\pi(\theta) \propto \exp\left\{ -\sum_{i=1}^n(y_i-\theta)^2/2 - \lambda |\theta|/2  \right\}.
$$


Therefore, the ''maximum a posteriori'' (MAP) estimator of $$\theta$$ can be expressed as

$$ \begin{eqnarray*} \widehat \theta^{MAP} &=& argmax_\theta \{\pi(\theta \mid y)\} = argmax_\theta \left\{\exp\left\{ -\sum_{i=1}^n(y_i-\theta)^2/2 - \lambda |\theta|/2  \right\} \right\} \\
&=& argmin_\theta\left\{ \sum_{i=1}^n(y_i-\theta)^2 + \lambda|\theta| \right\}\\
&=& \widehat\theta_{L_1}. \end{eqnarray*}$$

Hence, this shows that the $$ L_1 $$ penalized estimator and the MAP estimator based on  [the Laplace distribution](https://en.m.wikipedia.org/wiki/Laplace_distribution) are equivalent. However, the sharp density around the origin in the Laplace prior hinders the MCMC chain from mixing well. To overcome this issue,  [Park and Casella (2008)](http://www.stat.ufl.edu/~casella/Papers/Lasso.pdf) introduce a mixture of a Gaussian prior that is marginally equivalent to the Laplace prior as follows:

$$\begin{eqnarray*}
\theta \mid \tau^2 &\sim& N(0,\tau^2)\\
\tau^2 &\sim& exp(\lambda).
\end{eqnarray*}
$$


More specifically, 
$$ \begin{eqnarray*}
\pi(\theta) &\propto& \int \pi(\theta\mid \tau^2) \pi(\tau^2) d\tau\\
&\propto&\int \exp\{-\theta^2/(2\tau^2) - \lambda \tau^2 \} d\tau\\
&\propto&\exp\{  -(2\lambda)^{1/2}|\theta|)\}\\
&\sim& Laplace\left((2\lambda)^{1/2}\right).
\end{eqnarray*}
$$

Hence, in the Gibbs sampling, first it samples $$ \tau^2 $$ from  the conditional posterior distribution given $$ \theta $$, then it generates a posterior sample $$ \theta $$ given $$ \tau^2 $$, and it is very esay, since  the posterior distribution of $$ \theta $$ given $$ \tau^2 $$ is conjugate. The mixing of the resulting Gibbs sampling is much more efficient than the naive version of the Bayesian LASSO.

###The Bias of LASSO and the Tail Behavior of Bayesian LASSO

In this section, I am going to talk about the bias of $ L_1 $ penalty and its interpretation in a Bayesian perspective.

![look at me2](/assets/blasso.png)

