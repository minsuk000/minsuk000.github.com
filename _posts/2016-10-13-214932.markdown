---
layout: post
title: "Penalized Likelihood Estimator and Shrinkage Priors (3): LASSO and Bayesian LASSO"
date: 2016-10-13 21:47
comments: true
---

### A Toy Example

Consider the following simple model that has been used in the previous post ["Penalized Likelihood Estimator and Shrinkage Priros (1)"]({% post_url 2016-07-02-120519%}):

$$
y_i  = \theta + \epsilon_i,
$$

where $$\epsilon_i\sim N(0,1) $$ for $$i=1,\dots,n$$.

One might be interested in the sparsity of $$ \theta $$; whether $$ \theta $$ is zero or not. In this simple example, we are able to use a hypothesis testing to make an inference on $$ \theta $$, but when we consider multiple parameters, it is not clear to infer the sparsity. This issue has motivated a use of penalty on the likelihood function.

### LASSO and Bayesian LASSO
Consider a $$L_1 $$ penalized likelihood estimator as 

$$
\widehat\theta_{L_1} = {argmin}_\theta \left\{ -2\log L(y\mid \theta) + \lambda|\theta| \right\} = argmin_\theta\left\{ \sum_{i=1}^n(y_i-\theta)^2 + \lambda|\theta| \right\},
$$

where $$ L(y \mid \cdot) $$ is the likelihood function and $$ \lambda $$ is the tuning parameter. Then, the resulting estimator has an explicit form as 

$$
\widehat\theta_{L_1} = \begin{cases} \bar y -\frac{\lambda}{n}sgn(\bar y),\:\:\mbox{if}\:\:|\bar y| \geq \frac{\lambda}{n}\\
0,\:\:\mbox{otherwise}, 
\end{cases}  
$$

where $$\bar y $$ is the sample mean and $$sgn(\cdot) $$ is the sign function. The solution is plotted in the previous post ["Penalized Likelihood Estimator and Shrinkage Priors (1)"]({% post_url 2016-07-02-120519%}).
 
This $$L_1 $$ penalty idea can be applied to a linear regression model, then the penalized likelihood method is called the ''Least Absolute Shrinkage and Selection Operator'' ([LASSO](http://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents)). [Park and Casella (2008)](http://www.stat.ufl.edu/~casella/Papers/Lasso.pdf) proposed a natural Bayesian version of LASSO by imposing  [the Laplace distribution](https://en.m.wikipedia.org/wiki/Laplace_distribution) (or double-exponential distribution) on the regession parameter. The resulting posterior distribution of $$ \theta$$ in the above simple model can be expresses as 

$$
 \pi(\theta \mid y) \propto L(y\mid\theta)\pi(\theta) \propto \exp\left\{ -\sum_{i=1}^n(y_i-\theta)^2/2 - \lambda |\theta|/2  \right\}.
$$


Therefore, the ''maximum a posteriori'' (MAP) estimator of $$\theta$$ can be expressed as

$$ \begin{eqnarray*} \widehat \theta^{MAP} &=& argmax_\theta \{\pi(\theta \mid y)\} = argmax_\theta \left\{\exp\left\{ -\sum_{i=1}^n(y_i-\theta)^2/2 - \lambda |\theta|/2  \right\} \right\} \\
&=& argmin_\theta\left\{ \sum_{i=1}^n(y_i-\theta)^2 + \lambda|\theta| \right\}\\
&=& \widehat\theta_{L_1}. \end{eqnarray*}$$

Hence, this shows that the $$ L_1 $$ penalized estimator and the MAP estimator based on  [the Laplace distribution](https://en.m.wikipedia.org/wiki/Laplace_distribution) are equivalent. However, the sharp density around the origin in the Laplace prior hinders the MCMC chain from mixing well. To overcome this issue,  [Park and Casella (2008)](http://www.stat.ufl.edu/~casella/Papers/Lasso.pdf) introduce a mixture of a Gaussian prior that is marginally equivalent to the Laplace prior as follows:

$$\begin{eqnarray*}
\theta \mid \tau^2 &\sim& N(0,\tau^2)\\
\tau^2 &\sim& exp(\lambda).
\end{eqnarray*}
$$


More specifically, 

$$ \begin{eqnarray*}
\pi(\theta) &\propto& \int \pi(\theta\mid \tau^2) \pi(\tau^2) d\tau\\
&\propto&\int \exp\{-\theta^2/(2\tau^2) - \lambda \tau^2 \} d\tau\\
&\propto&\exp\{  -(2\lambda)^{1/2}|\theta|)\}\\
&\sim& Laplace\left((2\lambda)^{1/2}\right).
\end{eqnarray*}
$$

Hence, in the Gibbs sampling, first it samples $$ \tau^2 $$ from  the conditional posterior distribution given $$ \theta $$, then it generates a posterior sample $$ \theta $$ given $$ \tau^2 $$, and it is very esay, since  the posterior distribution of $$ \theta $$ given $$ \tau^2 $$ is conjugate. The mixing of the resulting Gibbs sampling is much more efficient than the naive version of the Bayesian LASSO.

### The Bias of LASSO and the Tail Behavior of Bayesian LASSO

In this section, I am going to talk about the bias of $$ L_1 $$ penalty and its interpretation in a Bayesian perspective.


![look at me3](/assets/blasso.png)

##### Figure (1) Penalty function of LASSO and prior dendisty of Bayesian LASSO.

The above figure illustrates the  $$L_1 $$ penalty function and the corresponding Bayesian prior (Laplace distribution). The problem of  $$L_1 $$ penalty is that as  $$\theta $$ gets larger the penalty on  $$\theta $$  also becomes stronger, which means that even when the MLE of  $$\theta $$ is large enough not to be penalized, the  $$L_1 $$  penalized estimator shrinks towards zero. [Fan and Li (2001)](http://orfe.princeton.edu/~jqfan/papers/01/penlike.pdf ) defined this overshrinking property as the "bias" of a penalty function. In a Bayesian point of view, on the other hand, this bias is closely related to the tail behavior of the prior corresponding to the penalty of the penalized likelihood. The tail of the Laplace density decays exponentially, since the density function is proportional to  $$ \exp\{ - \lambda \mid\theta\mid\} $$ for some constant $$\lambda$$. When the model contains a large number of parameters and one expects strong sparsity on the parameters, it is required to choose a large $$\lambda$$, then the resulting Laplace prior has scarce density at the tail part. This tail behavior results in overshrinkage effects towards zero so that the resulting posterior distribution is not able to concentrate around the true parameter value in the optimal rate.

### The Idea of Horseshoe Prior 
 
In the previous section, I discussed that the light tail behavior of the Bayesian LASSO prior is problematic. To avoid such a overshrinking phenomenon, the shrinkage prior requires the two properties:

* The prior density allocates a sufficiently large probability mass in a very small neighborhood of zero.
* The prior density has heavy tails; e.g., polynomial tails.

The Bayesian LASSO prior satisfies the first condition, but it fails to achieve the second one, since it has exponentially decaying tails. 

[Carvalho and Scott (2010)](http://faculty.chicagobooth.edu/nicholas.polson/research/papers/Horse.pdf) proposed the HorseShoe (HS) prior that attains the above two properties. It follows as

$$
\begin{eqnarray*}
\theta\mid \tau^2 &\sim& N(0,\tau^2)\\
\tau &\sim& Cauchy^+,
\end{eqnarray*}
$$

where $$  Cauchy^+ $$  is a standard half-Cauchy distributon. Then, the marginal prior density of $$ \theta $$  approximately can be shown  $$ \pi(\theta)\approx C_1\log (1+C_2/θ^2) $$ for some constants   $$ C_1 $$ and $$ C_2 $$. Also, one can show that $$\log (1+C_2/θ^2)\asymp \theta^{-2}$$ when $$\theta $$ is large enough, which means that the marginal HS prior has Cauchy-like tails.

The intuition of the HS prior can be explained by reparameterizing $$\omega = (1+\tau^2)^{-1}$$ as

$$
\begin{eqnarray*}
E[\theta \mid y, \omega] &=& (1-\omega)\bar y + \omega \cdot 0\\ 
\omega &\sim& Beta(1/2,1/2). 
 \end{eqnarray*}
$$

The conditional posterior expectation of $$ \theta  $$ is a convex mixture between the MLE $$ \bar y $$ and zero, and the weight $$\omega $$ has the prior density $$ Beta(1/2,1/2) $$.

![look](/assets/hs.png)

The above figure illustrates the density function of $$Beta(1/2,1/2)$$. The density of $$Beta(1/2,1/2)$$ assigns a large proportion of probability mass around zero and one, which results in  the posterior expectation of $$\theta $$ shrinking towards the $$\bar y  $$ or zero.

### Conclusion

 I have talked about a connection between LASSO and Bayesian LASSO, and also discussed a disadvantage of using the $$L_1$$ penalty or the Laplace prior in a Bayesian perspective. Instead, I prefer to use the horseshoe prior, but the computational mixing issue in MCMC is still 
 problematic. It is a big challenge to avoid computational complexity in high-dimensional Bayesian statistics.
 

