---
layout: post
title: "The Optimality Of Posterior Contraction Rate and Priors (2)"
comments: true
---

A few days ago, [Nurzhan Nurushev](http://www.math.vu.nl/en/about-the-department/people/phd-students/nurushevn.aspx) commented on my previous post ["The Optimality Of Posterior Contraction Rate and Priors"]({% post_url 2016-07-04-170943%}) at [J ISBA fabook group](https://www.facebook.com/groups/843187915745360/) as follows:

>Shin Minsuk , I think you are partly true. You missed the one case: if you consider gaussian prior(N(mu, sigma^2)) and use empirical Bayes approach wrt mu parameter, then you will avoid over-shrinkage problem. You can read this paper([http://arxiv.org/abs/1511.01803](http://arxiv.org/abs/1511.01803)). In particular, we obtained all consistency results for gaussian prior.
What he is saying is that some empirical Gaussian priors on $$\theta$$ can resolve the issue of using Gaussian tailed priors in the minimax posterior contraction rate.

Recall that the setting is $$y_i\mid \theta \overset{i.i.d.}{\sim}N(\theta,1)$$ for $$i=1,\dots,n$$. 

In the previous post, I mentioned that using Gaussian tailed priors could hurt the optimal minimax rate of posterior contraction, e.g., $$\theta \sim N(0,\tau^2)$$. So the resulting supremum of the risk of the posterior mean can be expressed as 

$$
\sup_{\theta_0} E_{\theta_0}[\{\theta_0 - E(\theta\mid y)\}^2] = \infty
$$

Instead, we can consider an empirical prior such as $$\theta \sim N(\bar y, \tau^2)$$, where $$\bar y$$ is the sample mean. This is not rigorous full Bayes prior, because the prior depends on the data. Then, the resulting risk is

$$
\begin{eqnarray*}
\sup_{\theta_0} E_{\theta_0}[\{\theta_0 - E(\theta\mid y)\}^2] = \frac{1}{n}.
\end{eqnarray*}
$$ 

Actually, their model in [the paper](http://arxiv.org/abs/1511.01803) is Gaussian sequence models that can be defined as 
$$
\begin{eqnarray*}
y_i\mid\theta_i \sim N(\theta_i,\sigma^2),
\end{eqnarray*}
$$ 
for $$i=1,\dots,n$$. Then, the prior that they consider is $$\theta\sim N(y_i,\sigma^2\tau^2)$$ for $$i\in {\bf k}$$ for some model $${\bf k}$$; $$\theta_i = 0$$ for $$i\not\in {\bf k}$$. Because the prior mean is the data itself, it could avoid *over-shrinkage* effects. It is simple, but interesting.

