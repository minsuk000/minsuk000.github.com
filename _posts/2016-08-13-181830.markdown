---
layout: post
title: "Penalized likelihood Estimator and Shrinkage Priors (2): Hodge's Estimator and Super-efficiency"
comments: true
---
 
 Previously, I talked about the sparse and shrinkage estimator based on penalized likelihood methods (["Penalized likelihood Estimator and Shrinkage Priors (1)"]({% post_url 2016-07-02-120519%})). Today I am going to discuss the legendary estimator so-called "Hodge's estimator" and its theoretical properties.

The legendary statistician R. A. Fisher believed that the maximum likelihood estimators (MLE) would be the most efficient in the sense that the variance of the MLE is always less or equal to that of any other estimator, at least in an asymptotic sense. However, Hodges' famous example of [super-efficiency](http://www.stat.yale.edu/~pollard/Books/LeCamFest/VanderVaart.pdf) not only broke down Fisher's naive belief, but also brought a huge shock to the world of statistics, since Hodges showed that his estimator achieves less asymptotic variance than that of MLE when the true parameter is exactly same as the specified test value, and that it is as efficient as MLE otherwise.

The example that he provided was a simple one as follows:

$$
y_i \sim N(\theta, 1),
$$  

where $$\theta$$ is an unknown parameter for $$i = 1,\dots,n$$. It is well-known that the MLE is the sample  mean $$\bar y$$ and $$n^{1/2}(\bar y - \theta)$$ converges to $$N(0,1)$$ in distribution, as $$n$$ goes to infinity. Then, Hodges' estimator $$\widehat \theta^H_n$$ can be defined as a hard-thresholded estimator based on the MLE with the threshold $$n^{-1/4}$$; *i.e.*,

$$
\widehat \theta^H_n := \begin{cases}
\bar y, \:\:\mbox{if}\:|\bar y|\geq n^{-1/4}\\
0, \:\:\mbox{if}\:|\bar y|< n^{-1/4}.
\end{cases}
$$ 

In the provided settings, Hodges' estimator is "super-efficient" in a sense that it is not only consistent in estimation, but its asymptotic expected square error is also never worse than that of the MLE; *i.e.*, under $$\theta\neq 0, $$ $$n^{1/2}(\widehat\theta^H_n-\theta)$$ converges to $$N(0,1)$$ in distribution, as $$n$$ tends to infinity, under $$\theta=0$$, $$n^{1/2}(\widehat\theta^H_n-\theta)$$ converges to a degenerate distribution of zero. In other words, it asymptotically outperforms MLE at $$\theta=0$$ and is not worse than MLE at $$\theta\neq 0$$. 

### There is no free lunch!

  Although Hodges' estimator attains super-efficiency in some settings, [Le Cam (1953)](https://books.google.com/books/about/On_some_asymptotic_Properties_of_maximum.html?id=_SqVQwAACAAJ) showed that the MLE is optimal among regular estimators, which means that the asymptotic behavior of the estimator is smooth with respect to the parameter $$\theta$$. He also showed that the set of parameter $$\theta$$ where Hodges' estimator is more efficient than the MLE must have Lebesgue measure zero. 
  
  Moreover, [Leeb and Potscher (2008)](https://ideas.repec.org/p/cwl/cwldpp/1500.html) provided that any sparse estimator can have a maximal risk diverging to infinity whenever the loss function is unbounded. In other words, sparse estimators compromise the risk near zero to take advantage of the usefulness of sparsity. As the proverb goes, *there is no free lunch*.
   

 
