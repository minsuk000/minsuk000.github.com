---
layout: post
title: "How to prove the  Bayesian Model Selection Consistency in Linear Models"
comments: true
---
 
 Today I am going to talk about some theory about Bayesian model selection. Consider a linear regression model given a model $${\bf k}$$ as 
 
$$
 y = X_{\bf k}\beta_{\bf k} + \epsilon,
 $$
 
 where $$\epsilon \sim N(0,\sigma^2I_n)$$.  We assume that only a fixed number of parameters  are non-zero, and note the size by $$s_0$$. Also, let the set of indices of the true non-zero parameters denoted by $${\bf  t}$$. For simplicity, the prior of parameters is a discrete mixture of a Gaussian with a zero mean and a variance $$\tau^2$$; i.e., $$\beta_j\sim N(0,\sigma^2\tau^2)$$ for $$j=1,\dots,p$$. Even though Gaussian tails hurt the optimal minimax rate of posterior contraction, it is ok when we only consider model selections. The resulting posterior probability of each model  is 

$$
\pi({\bf k}\mid y) = \frac{m_{\bf k}(y)\pi({\bf k})}{\sum_{\bf l}m_{\bf l}(y)\pi({\bf l})}, 
$$  


 where $$m_{\bf k}(y)$$ is the marginal likelihood of the model $${\bf k}$$ and $$\pi({\bf k})$$ is its model prior. Consider the model prior as
 
$$
\pi({\bf k})\propto p^{-\lambda|{\bf k}|},
$$


 where $$ \vert {\bf k} \vert$$ is the model size of a model $${\bf k}$$.
 
The Bayesian model selection consistency is
  defined by
  
  $$
  \pi({\bf t}\mid y)\overset{p}{\to} 1.
  $$
 
  It looks very simple. The posterior probability of the true model converges to one in probability. A brief of  the proof of the consistency follows as 
 
$$\begin{eqnarray*}
\pi({\bf t}\mid y) &=&  \frac{m_{\bf t}(y)\pi({\bf t})}{\sum_{\bf k}m_{\bf k}(y)\pi({\bf k})}\\
 &=& \left(1+\sum_{ {\bf k} : {\bf  t} \subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})} + \sum_{ {\bf k} : {\bf  t} \not \subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})}\right)^{-1}. 
  \end{eqnarray*}
$$


Then, it is sufficient to show that i) $$\sum_{ {\bf k} : {\bf  t} \subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})}\overset{p}{\to}0$$ and ii) $$\sum_{ {\bf k} : {\bf  t} \not\subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})}\overset{p}{\to}0$$. The second one (under-fitting models case) is relatively easy to show, because models that miss any important variables will be exponentially penalized. Therefore, we focus on the first statement regarding over-fitting models. The commonly used technique is following as 

$$\begin{eqnarray*}
&&P\left[ \sum_{ {\bf k} : {\bf t} \subset {\bf k} }\frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})} >\epsilon \mid y \right]\\
&\approx& P\left[ \sum_{ {\bf k} : {\bf t} \subset {\bf k} } p^{-\lambda(|{\bf k}| -|{\bf t}| )}\tau^{-(|{\bf k}| -|{\bf t}| )}\left\{\frac{X_{\bf k}^TX_{\bf k}+1/\tau^2I}{X_{\bf t}^TX_{\bf t}+1/\tau^2I}\right\}^{-1/2}\exp\{y^T(P_{\bf k}-P_{\bf t})y/(2\sigma^2) \} >\epsilon \mid y \right]\\
&=&  P\left[ \sum_{d=1}^{ p-| {\bf t} | } \sum_{ {\bf k} : {\bf k} \subset {\bf t} \& | {\bf t} \setminus {\bf k} | = d } p^{-\lambda d}\tau^{-d}\left\{\frac{X_{\bf k}^TX_{\bf k}+1/\tau^2I}{X_{\bf t}^TX_{\bf t}+1/\tau^2I}\right\}^{-1/2} \exp\{y^T(P_{\bf k}-P_{\bf t})y/(2\sigma^2) \} >\epsilon \mid y \right]\\
 &\leq& \sum_{ d=1 }^{ p-| {\bf t} | } P\left[  \sum_{ {\bf k} : {\bf t} \subset{\bf k} \& | {\bf k} \setminus {\bf t} | = d } p^{-\lambda d}\tau^{-d} \left\{\frac{X_{\bf k}^TX_{\bf k}+1/\tau^2I}{X_{\bf t}^TX_{\bf t}+1/\tau^2I}\right\}^{-1/2} \exp\{y^T(P_{\bf k}-P_{\bf t})y/(2\sigma^2) \}  > \epsilon/(p-|{\bf t}|) \mid y \right], 
 \end{eqnarray*} 
 $$
 
where $$P_{ \bf k }$$ is the projection matrix of $$X_{ {\bf k} }$$, and $${{\bf k} \setminus {\bf t}}$$ is the model $${\bf k}$$ without variable in $${\bf t}$$. The proof will be completed, if we can show that 

asdfas
