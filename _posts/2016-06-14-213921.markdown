---
layout: post
title: "How to prove the  Bayesian Model Selection Consistency in Linear Models"
comments: true
---
 
 Today I am going to talk about some theory about Bayesian model selection. Consider a linear regression model given a model $${\bf k}$$ as 
 
$$
 y = X_{\bf k}\beta_{\bf k} + \epsilon,
 $$
 
 where $$\epsilon \sim N(0,\sigma^2I_n)$$.  We assume that only a fixed number of parameters  are non-zero, and note the size by $$s_0$$. Also, let the set of indices of the true non-zero parameters denoted by $${\bf  t}$$. For simplicity, the prior of parameters is a discrete mixture of a Gaussian with a zero mean and a variance $$\tau^2$$; i.e., $$\beta_j\sim N(0,\sigma^2\tau^2)$$ for $$j=1,\dots,p$$. Even though Gaussian tails hurt the optimal minimax rate of posterior contraction, it is ok when we only consider model selections. The resulting posterior probability of each model  is 

$$
\pi({\bf k}\mid y) = \frac{m_{\bf k}(y)\pi({\bf k})}{\sum_{\bf l}m_{\bf l}(y)\pi({\bf l})}, 
$$  


 where $$m_{\bf k}(y)$$ is the marginal likelihood of the model $${\bf k}$$ and $$\pi({\bf k})$$ is its model prior. Consider the model prior as
 
$$
\pi({\bf k})\propto p^{-\lambda|{\bf k}|},
$$


 where $$ \vert {\bf k} \vert$$ is the model size of a model $${\bf k}$$.
 
The Bayesian model selection consistency is
  defined by
  
  $$
  \pi({\bf t}\mid y)\overset{p}{\to} 1.
  $$
 
  It looks very simple. The posterior probability of the true model converges to one in probability. A brief of  the proof of the consistency follows as 
 
$$\begin{eqnarray*}
\pi({\bf t}\mid y) &=&  \frac{m_{\bf t}(y)\pi({\bf t})}{\sum_{\bf k}m_{\bf k}(y)\pi({\bf k})}\\
 &=& \frac{1}{1+\sum \frac{m_{\bf k})(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})} }. 
  \end{eqnarray*}
$$
