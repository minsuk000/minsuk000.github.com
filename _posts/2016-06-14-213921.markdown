---
layout: post
title: "How to prove the  Bayesian Model Selection Consistency in Linear Models"
comments: true
---
 
 Today I am going to talk about some theory about Bayesian model selection. Consider a linear regression model given a model $${\bf k}$$ as 
 
$$
 y = X_{\bf k}\beta_{\bf k} + \epsilon,
 $$
 
 where $$\epsilon \sim N(0,\sigma^2I_n)$$.  We assume that only a fixed number of parameters  are non-zero, and note the size by $$s_0$$. Also, let the set of indices of the true non-zero parameters denoted by $${\bf  t}$$. For simplicity, the prior of parameters is a discrete mixture of a Gaussian with a zero mean and a variance $$\tau^2$$; i.e., $$\beta_j\sim N(0,\sigma^2\tau^2)$$ for $$j=1,\dots,p$$. Even though Gaussian tails hurt the optimal minimax rate of posterior contraction, it is ok when we only consider model selections. The resulting posterior probability of each model  is 

$$
\pi({\bf k}\mid y) = \frac{m_{\bf k}(y)\pi({\bf k})}{\sum_{\bf l}m_{\bf l}(y)\pi({\bf l})}, 
$$  


 where $$m_{\bf k}(y)$$ is the marginal likelihood of the model $${\bf k}$$ and $$\pi({\bf k})$$ is its model prior. Consider the model prior as
 
$$
\pi({\bf k})\propto p^{-\lambda|{\bf k}|},
$$


 where $$ \vert {\bf k} \vert$$ is the model size of a model $${\bf k}$$.
 
The Bayesian model selection consistency is
  defined by
  
  $$
  \pi({\bf t}\mid y)\overset{p}{\to} 1.
  $$
 
  It looks very simple. The posterior probability of the true model converges to one in probability. A brief of  the proof of the consistency follows as 
 
$$\begin{eqnarray*}
\pi({\bf t}\mid y) &=&  \frac{m_{\bf t}(y)\pi({\bf t})}{\sum_{\bf k}m_{\bf k}(y)\pi({\bf k})}\\
 &=& \left(1+\sum_{ {\bf k} : {\bf  t} \subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})} + \sum_{ {\bf k} : {\bf  t} \not \subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})}\right)^{-1}. 
  \end{eqnarray*}
$$


Then, we have to show that i) $$\sum_{ {\bf k} : {\bf  t} \subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})}\overset{p}{\to}0$$ and ii) $$\sum_{ {\bf k} : {\bf  t} \not\subset {\bf k} } \frac{m_{\bf k}(y)\pi({\bf k})}{m_{\bf t}(y)\pi({\bf t})}\overset{p}{\to}0$$. The second one (under-fitting models case) is relatively easy to show, because models that miss any important variables will be exponentially penalized. Therefore, we focus on the first statement regarding over-fitting models. The commonly used technique is following as 

